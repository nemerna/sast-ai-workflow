import os
import time
import logging

from langchain_community.vectorstores import FAISS
from transformers import AutoTokenizer
from Utils.file_utils import read_all_source_code_files
from common.constants import *

logger = logging.getLogger(__name__)

def generate_code_embeddings(llm_service):
    if os.path.exists("./../faiss_index/index.faiss"):
        logger.info("Loading source code embeddings from file index")
        src_db = FAISS.load_local("./../faiss_index", llm_service.embedding_llm,
                                  allow_dangerous_deserialization=True)
    else:
        code_text = read_all_source_code_files()
        src_text = code_text if len(code_text) > 0 else []
        start = time.time()
        logger.info("Creating embeddings for source code...")
        src_db = llm_service.create_vdb(src_text)
        src_db.save_local("./../faiss_index")
        end = time.time()
        logger.info(f"Src project files have embedded completely. It took : {end - start} seconds")

    return src_db

def check_text_size_before_embedding(text: str, model_name: str):
    """
    Checks if the text exceeds the maximum allowed tokens for the embedding model.
    Print a warning if the text is too long.
    """
    # Load the tokenizer for the embedding model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    max_tokens = tokenizer.model_max_length
    
    # Count tokens
    tokens = tokenizer(text)
    token_count = len(tokens["input_ids"])
    
    if token_count > max_tokens:
        logger.warning(
            f"WARNING: Text length is {token_count} tokens, exceeding the max allowed ({max_tokens}). "
            f"\nFirst 20 words of the text: {text.split()[:20]}"
        )
    # else:
    #     logger.info(f"Text is within limit: {token_count} tokens.")